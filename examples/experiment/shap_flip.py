# -*- coding: utf-8 -*-
"""fl_lfa_shap.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1O_1ZSuOIHl8TYX3HmHRkHyzysE_uGJ7m
"""

#!pip install shap
import argparse
import numpy as np
import pandas as pd
import pickle
import copy
import torch
import torchvision
import matplotlib.pyplot as plt
from time import time
from torchvision import datasets,transforms
from torch import optim
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset, TensorDataset
import os
import random
from tqdm import tqdm
import torch.nn.functional as F
from collections import Counter
from itertools import islice
import shap

def load_dataset():
  train_data = datasets.MNIST(root='./data',train=True,transform=transform,download=True)
  test_data = datasets.MNIST(root='./data',train=False,transform=transform,download=True)
  return train_data, test_data

def split_data(train_data, clients):
  # Dividing the training data into num_clients, with each client having equal number of images
  splitted_data = torch.utils.data.random_split(train_data, [int(train_data.data.shape[0] / clients) for _ in range(clients)])
  return splitted_data

def split_label_wise(train_data):
    label_wise_data = []
    for i in range(10):
        templabeldata = []
        j = 0
        for instance, label in train_data:
            if label == i:
                templabeldata.append(train_data[j])
            j += 1
        label_wise_data.append(templabeldata)
        
    return label_wise_data

def load(train_data, test_data):
  train_loader = [torch.utils.data.DataLoader(x, batch_size=batch_size, shuffle=True) for x in train_data]
  test_loader = torch.utils.data.DataLoader(test_data, batch_size = batch_size, shuffle=True) 

  return train_loader, test_loader

# declare a transformation for MNIST

transform=transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
        ])

# neural network architecture declaration

class Model_MNIST(nn.Module):
  def __init__(self):
    super(Model_MNIST, self).__init__()
    self.conv1 = nn.Conv2d(1, 32, 3, 1)
    self.conv2 = nn.Conv2d(32, 64, 3, 1)
    self.dropout1 = nn.Dropout2d(0.25)
    self.dropout2 = nn.Dropout2d(0.5)
    self.fc1 = nn.Linear(9216, 128)
    self.fc2 = nn.Linear(128, 10)


  def forward(self,x):
    x = self.conv1(x)
    x = F.relu(x)
    x = self.conv2(x)
    x = F.relu(x)
    x = F.max_pool2d(x, 2)
    x = self.dropout1(x)
    x = torch.flatten(x, 1)
    x = self.fc1(x)
    x = F.relu(x)
    x = self.dropout2(x)
    x = self.fc2(x)
    output = F.log_softmax(x, dim=1)
    return output

# Model for SHAP neural network architecture declaration

class Model_Shap(nn.Module):
    def __init__(self):
        super(Model_Shap, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, 3, 1)
        self.conv2 = nn.Conv2d(32, 64, 3, 1)
        self.dropout1 = nn.Dropout2d(0.25)
        self.dropout2 = nn.Dropout2d(0.5)
        self.fc1 = nn.Linear(9216, 256)
        self.fc2 = nn.Linear(256, 100)
    
    def forward(self,x):
        x = self.conv1(x)
        x = F.relu(x)
        x = self.conv2(x)
        x = F.relu(x)
        x = F.max_pool2d(x, 2)
        x = self.dropout1(x)
        x = torch.flatten(x, 1)
        x = self.fc1(x)
        x = F.relu(x)
        x = self.dropout2(x)
        x = self.fc2(x)
        output = F.log_softmax(x, dim=1)
        return output

# client training process


# epoch = local epochs at each client (E)
# batch_idx = local minibatch size at each client (B)


def client_update(current_local_model, train_loader, optimizer, epoch):

    current_local_model.train()

    for e in range(epoch):
      running_loss = 0
      for batch_idx, (data, target) in enumerate(train_loader):
        optimizer.zero_grad()
        output = current_local_model(data)
        loss = F.nll_loss(output, target)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
      #print("Epoch {} - Training loss: {}".format(e,running_loss/len(train_loader)))

    # return client update
    return loss.item()

# aggregation at server


# saving the global_model state_dict
# concatenate the sequence of tensors of client models along 0 diemnsion and taking average
# load updated global model in client models

def server_aggregate(global_model, client_models):
  
    # print current global model
    #for param in global_model.parameters():
    # print(global_model.data)
    
    # aggregate  
    global_dict = global_model.state_dict()   
    for k in global_dict.keys():
        global_dict[k] = torch.stack([client_models[i].state_dict()[k].float() for i in range(len(client_models))], 0).mean(0)
    global_model.load_state_dict(global_dict)
    for model in client_models:
        model.load_state_dict(global_model.state_dict())

def test(model, test_loader, actual_prediction, target_prediction):
    print("Testing")
    model.eval()
    test_loss = 0
    correct = 0
    attack_success_count = 0
    instances = 0
    misclassifications = 0
    with torch.no_grad():
        for data, target in test_loader:
            #print(len(target))
            output = model(data)
            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss
            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability
            #print(len(pred))
            #i = 0
            #print("actual labels : ", target)
            #print("predicted labels : ",pred)
            for i in range(len(target)):
              if target[i] == actual_prediction:
                instances += 1
              if target[i] != pred[i]:  
                if target[i] == actual_prediction and pred[i] == target_prediction:
                  attack_success_count += 1
                else:
                  misclassifications += 1
                #i += 1
            correct += pred.eq(target.view_as(pred)).sum().item()

    test_loss /= len(test_loader.dataset)
    acc = correct / len(test_loader.dataset)

    attack_success_rate = attack_success_count/instances
    attack_success_rate *= 100
    misclassification_rate = misclassifications/instances

    print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
        test_loss, correct, len(test_loader.dataset), 100* acc ))
    print('Test Samples with target label {} : {}'.format(actual_prediction,instances))
    print('Test Samples predicted as  {} : {}'.format(target_prediction,attack_success_count))
    print('Test Samples with target label {} misclassified : {}'.format(actual_prediction,misclassifications))
    print("Attack success rate",attack_success_rate)
    print("misclassification_rate", misclassification_rate)
    return test_loss, acc , attack_success_rate, misclassification_rate

#  print the count of label in the data


def getcount_label(data):
  counts = dict()
  for instance,label in data:
    counts[label] = counts.get(label, 0) + 1

  for key, value in counts.items():
    print(key, ':' , value)

# poison client data by flipping labels  
# -1 : poison all labels

def poison_label(client_id, sourcelabel, targetlabel, count_poison, client_data):
  label_poisoned = 0
  client_data[client_id] = list(client_data[client_id])
  i = 0 
  for instance,label in client_data[client_id]:
    client_data[client_id][i] = list(client_data[client_id][i])
    if client_data[client_id][i][1] == sourcelabel:
      client_data[client_id][i][1] = targetlabel
      label_poisoned += 1
    client_data[client_id][i] = tuple(client_data[client_id][i])
    if label_poisoned >= count_poison and count_poison != -1:
      break
    i += 1
  client_data[client_id] = tuple(client_data[client_id])
  return label_poisoned

shap_dataset = []

def train(num_clients, num_rounds, train_loader, test_loader, losses_train, losses_test, 
          acc_train, acc_test, misclassification_rates, attack_success_rates,communication_rounds, clients_local_updates, global_update,
          source,target):
  # Initialize model and Optimizer

  # Initialize model
  global_model = Model_MNIST()
  global_model_copy = copy.copy(global_model)
  # create K (num_clients)  no. of client_models 
  client_models = [ Model_MNIST() for _ in range(num_clients)]

  # synchronize with global_model
  for model in client_models:
      model.load_state_dict(global_model_copy.state_dict()) # initial synchronizing with global model 

  # create optimizers for client_models
  optimizer = [optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5) for model in client_models]


  # List containing info about learning 

  # Runnining FL
  #attack_success_rate = 0
 
  for r in range(num_rounds):
      # client update
      loss = 0
      for i in tqdm(range(num_clients)):
          loss += client_update(client_models[i], train_loader[i],optimizer[i], epoch=epochs)


      
      shap_tr_loader = torch.utils.data.DataLoader(shap_background, batch_size = 512, shuffle=True) 
      # since shuffle=True, this is a random sample of test data
      batch = next(iter(shap_tr_loader))
      images, lables = batch
      print(images.size())
      print(lables.size())
      background = images[:450]
      img = images[510:]
      l = lables[510:]

      if r < 2:
        # calculate shap values
        id = 0
        print('calculating shap in round {}'.format(r+1))
        for model in client_models:
          print("starting client {} internal model train".format(id))
          e = shap.DeepExplainer(model, background)
          print("internal model train finished")
          print('generating data using client model {}'.format(id))
          for index in range(len(img)):
            shap_val = e.shap_values(img[index:index+1])
            for i in range(len(shap_val)):
                shap_dataset.append((shap_val[i][0],10*l[index]+i))
                #print('appended with label {}'.format(10*l[index]+i))
          print('Data generation finished using client model{}'.format(id))
          id += 1
        print(len(shap_dataset))
      elif r == 2:
        print('Training shap model in round {}'.format(r+1))
        # defining the model
        model = Model_Shap().float()
        # defining the optimizer
        optimizer_shap = optim.Adam(model.parameters(), lr=0.001)
        # defining the loss function
        criterion = nn.CrossEntropyLoss()
        model.train()

        howManyNumbers = int(round(0.9*len(shap_dataset)))
        train_shap = shap_dataset[:howManyNumbers]
        test_shap = shap_dataset[howManyNumbers:]
        #train_shap, test_shap = torch.utils.data.random_split(shap_dataset, [4500, 500])
        # defining trainloader and testloader
        trloader = torch.utils.data.DataLoader(train_shap, batch_size=32, shuffle=True)
        teloader = torch.utils.data.DataLoader(test_shap, batch_size=32, shuffle=True)

        for e in range(2):
          running_loss = 0
          for batch_idx, (data, trget) in enumerate(trloader):
            optimizer_shap.zero_grad()
            output = model(data.float())
            loss = criterion(output, trget)
            loss.backward()
            optimizer_shap.step()
            running_loss += loss.item()
          print("Epoch {} - Training loss: {}".format(e,running_loss/len(trloader)))

        # getting predictions on test set and measuring the performance
        correct_count, all_count = 0, 0
        for images,labels in teloader:
          for i in range(len(labels)):
            img = images[i].view(1, 1, 28, 28)
            with torch.no_grad():
                logps = model(img.float()) 
            ps = torch.exp(logps)
            probab = list(ps.cpu()[0])
            pred_label = probab.index(max(probab))
            true_label = labels.cpu()[i]
            if(true_label == pred_label):
              correct_count += 1
            all_count += 1

        print("Number Of Images Tested =", all_count)
        print("\nModel Accuracy =", (correct_count/all_count)*100)
      else:
        print('evaluating shap in round {}'.format(r+1))
        shap_te_loader = torch.utils.data.DataLoader(shap_test, batch_size = 512, shuffle=True) 
        batch = next(iter(shap_te_loader))
        images, lables = batch
        print(images.size())
        print(lables.size())
        background = images[:450]
        img = images[500:505]
        l = lables[500:505]
        id = 0
        for models in client_models:
          print('evaluating client model {}'.format(id))
          print("internal model train started")
          e = shap.DeepExplainer(model, background)
          print("internal model train finished")
          print('Calculating shap values using client model {}'.format(id))
          for index in range(len(img)):
            shap_val = e.shap_values(img[index:index+1])
            #print('original image label : {}'.format(l[index]))
            temp = []
            for i in range(len(shap_val)):
              im = torch.tensor(shap_val[i][0])
              with torch.no_grad():
                  logps = model(im.unsqueeze(0).float())
              ps = torch.exp(logps)
              probab = list(ps.cpu()[0])
              pred_label = probab.index(max(probab))
              temp.append(pred_label)
              #print(pred_label)
            print('original image label : {}'.format(l[index]))
            print("predictions : ",temp)
          id += 1


      
      #check euclidean distance

      print("checking Euclidean distances")
      for i in range(num_clients-1):
        for j in range(i+1,num_clients):
          print('distance b/w client{} and client{} : {}'.format(i,j,euclidean_distance(client_models[i],client_models[j])))

      #append clinet models and global models at the start of every round

      temp_updates_clients = []
      for i in range(num_clients):
        temp_updates_clients.append(copy.copy(client_models[i]))

      clients_local_updates.append(temp_updates_clients)
      global_update.append(global_model)


      losses_train.append(loss)
      communication_rounds.append(r+1)

      # server aggregate
      server_aggregate(global_model, client_models)

      # calculate test accuracy after the current round
      test_loss, acc ,asr, mcr = test(global_model, test_loader, source, target)
      losses_test.append(test_loss)
      acc_test.append(acc)
      misclassification_rates.append(mcr)
      attack_success_rates.append(asr)
      print("attack success rate : ",asr)
      print("misclassification rate ",mcr)
      #attack_success_rate = asr
      

      print('%d-th round' % (r+1))
      print('average train loss %0.3g | test loss %0.3g | test acc: %0.3f' % (loss / num_clients, test_loss, acc))

  #return attack_success_rate

# federated learning parameters

#num_clients = 10         # total number of clients (K)
#num_selected = 6         #  m no of clients (out of K) are selected at radom at each round
#num_rounds = 3
epochs = 2              # number of local epoch
batch_size = 32          # local minibatch size
learning_rate = 0.01       # local learning rate

def poisoning(client_data, attackers_id, count):
  res_count = count
  id = 0
  while True:
    if res_count > 0 and id < len(attackers_id):
      poisoned = poison_label(attackers_id[id],8,4,res_count,client_data)
      res_count -= poisoned
      id +=1
    else:
      break

def distribute_data_in_clients(label_wise_data):
    
    clients_data = []
    for i in range(10):
        clients_data.append([])

    dist = [[300,323,400,500,500,2000,500,500,400,500],
            [1000,500,500,500,1000,500,1000,742,500,500],
            [500,458,1000,500,500,500,500,500,500,1000],
            [500,1000,500,500,1000,500,500,1000,500,131],
            [2000,500,500,500,100,100,100,20,22,2000],
            [200,200,100,1800,100,100,200,221,2000,500],
            [500,500,1000,300,300,1000,400,500,418,1000],
            [1000,1000,1000,500,500,500,500,500,265,500],
            [900,450,450,900,225,226,900,900,450,450],
            [500,500,200,200,3000,500,500,249,100,200]]
   
    #print(len(dist))
    for lable in range(len(dist)):
        #print("label:",lable)
        loc = 0
        i = 0
        for ele in dist[lable]:
            for j in range(loc,loc+ele):
                #print(j)
                clients_data[i].append(label_wise_data[lable][j])
            i += 1
            loc  += ele
                        
    return clients_data

def euclidean_distance(model1,model2):
  # calculating euclidean distance
  d = 0
  for param1, param2 in zip(model1.parameters(),model2.parameters()):
    if len(list(param1.shape)) != 1 and len(list(param2.shape)) != 1:
      temp = torch.cdist(param1, param2, p=2)
      d += torch.norm(temp)
      #print(temp)
  print(d)

def run(attackers_id, source_label, poisoned_label,sample_to_poison,client_data, test_data):
  participated_clients = 5
  no_rounds = 2
  total_poisoned_samples = 0
  res_count = sample_to_poison
  #id = 0
  
  '''
  while True:
    if res_count > 0 and id < len(attackers_id):
      poisoned = poison_label(attackers_id[id],source_label,poisoned_label,res_count,client_data)
      total_poisoned_samples += poisoned
      res_count -= poisoned
      id +=1
    else:
      break
  '''

  for id in attackers_id:
    total_poisoned_samples += poison_label(id,source_label,poisoned_label,sample_to_poison,client_data)


  print("samples poisoned: ", total_poisoned_samples)
  train_loader, test_loader = load(client_data, test_data)
  losses_train_p = []
  losses_test_p = []
  acc_train_p = []
  acc_test_p = []
  communication_rounds_p = []
  clients_local_updates_p = []
  global_update_p = []
  misclassification_rates_p = []
  attack_success_rates_p = []
  #attack_success_rate = train(participated_clients,no_rounds,train_loader,test_loader,losses_train_p,losses_test_p,
      #acc_train_p,acc_test_p,misclassification_rates,attack_success_rates,communication_rounds_p,clients_local_updates_p,global_update_p,source_label,poisoned_label)
  

  train(participated_clients,no_rounds,train_loader,test_loader,losses_train_p,losses_test_p,
      acc_train_p,acc_test_p,misclassification_rates_p,attack_success_rates_p,communication_rounds_p,clients_local_updates_p,global_update_p,source_label,poisoned_label)

  print("accuracy",acc_test_p[len(acc_test_p)-1])
  return total_poisoned_samples, attack_success_rates_p, misclassification_rates_p ,acc_test_p, global_update_p, clients_local_updates_p,  communication_rounds_p

#attackers_id_list = []
#attackers_id_list.append([])
#attacker_id = []
#attackers_id_list.append(attacker_id)
#attacker_id = [1,2]
#attackers_id_list.append(attacker_id)
'''
attacker_id = [1,3]
attackers_id_list.append(attacker_id)
attacker_id = [4,7,8]
attackers_id_list.append(attacker_id)
'''

global_poison_sample_list = []
global_attack_success_rates_list = []
global_accuracy_list = []
global_client_updates = []
global_global_models = []
global_communication_rounds = []
global_misclassification_rates = []

train_data, test_data = load_dataset()
test_data_1, test_data_2 = torch.utils.data.random_split(test_data, [1000, 9000])
label_wise_data = split_label_wise(train_data)
#clients_data = copy.copy(label_wise_data)
clients_data = distribute_data_in_clients(label_wise_data)

shap_background, shap_test = torch.utils.data.random_split(test_data_2, [4000, 5000])

print("Deatils of process till now")
print("Poison sample list : ",global_poison_sample_list)
print("Attack success rates :",global_attack_success_rates_list)
print("Accuracy lists : ",global_accuracy_list)
print("misclassifications : ",global_misclassification_rates)

print("Running Baseline Federated Learning")
local_data_fl = copy.copy(clients_data)
attackers = [2]
poisoned_sample, attack_success_rate, misclassification_rates,acc_test, global_updates, client_local_updates, rounds = run(attackers,6,2,-1,local_data_fl, test_data_1)
global_accuracy_list.append(acc_test)
global_communication_rounds.append(rounds)
global_poison_sample_list.append(poisoned_sample)
global_attack_success_rates_list.append(attack_success_rate)
global_misclassification_rates.append(misclassification_rates)

global_client_updates.append(client_local_updates)
print("Summary")
print("No. of attackers", len(attackers))
print("No. of poisonous samples", poisoned_sample)
print("After training accuracy",acc_test[len(acc_test)-1])
print("Attack success rate", attack_success_rate)

# checking for shap
print(len(shap_dataset))

howManyNumbers = int(round(0.8*len(shap_dataset)))
train_shap = shap_dataset[:howManyNumbers]
test_shap = shap_dataset[howManyNumbers:]

print(len(test_shap))

# defining the model
model = Model_Shap().float()
# defining the optimizer
optimizer = optim.Adam(model.parameters(), lr=0.001)
# defining the loss function
criterion = nn.CrossEntropyLoss()
model.train()

howManyNumbers = int(round(0.9*len(shap_dataset)))
train_shap = shap_dataset[:howManyNumbers]
test_shap = shap_dataset[howManyNumbers:]
#train_shap, test_shap = torch.utils.data.random_split(shap_dataset, [2800, 400])
# defining trainloader and testloader
trloader = torch.utils.data.DataLoader(train_shap, batch_size=32, shuffle=True)
teloader = torch.utils.data.DataLoader(test_shap, batch_size=32, shuffle=True)

for e in range(2):
  running_loss = 0
  for batch_idx, (data, target) in enumerate(trloader):
    optimizer.zero_grad()
    output = model(data.float())
    loss = criterion(output, target)
    loss.backward()
    optimizer.step()
    running_loss += loss.item()
  print("Epoch {} - Training loss: {}".format(e,running_loss/len(trloader)))

# getting predictions on test set and measuring the performance
correct_count, all_count = 0, 0
for images,labels in teloader:
  for i in range(len(labels)):
    img = images[i].view(1, 1, 28, 28)
    with torch.no_grad():
        logps = model(img.float()) 
    ps = torch.exp(logps)
    probab = list(ps.cpu()[0])
    pred_label = probab.index(max(probab))
    true_label = labels.cpu()[i]
    if(true_label == pred_label):
      correct_count += 1
    all_count += 1

print("Number Of Images Tested =", all_count)
print("\nModel Accuracy =", (correct_count/all_count)*100)

ms = global_client_updates[0][0][1]

shap_tr_loader = torch.utils.data.DataLoader(shap_background, batch_size = 512, shuffle=True) 
# since shuffle=True, this is a random sample of test data
batch = next(iter(shap_tr_loader))
images, lables = batch
print(images.size())
print(lables.size())
background = images[:450]
img = images[500:503]
l = lables[500:503]

e = shap.DeepExplainer(ms, background)
print("internal model train finished")
print('generating data using client model {}'.format(id))
for index in range(len(img)):
  shap_val = e.shap_values(img[index:index+1])
  print('original label : {}'.format(l[index]))
  for i in range(len(shap_val)):
    im = torch.tensor(shap_val[i][0])
    with torch.no_grad():
        logps = model(im.unsqueeze(0).float())
    ps = torch.exp(logps)
    probab = list(ps.cpu()[0])
    pred_label = probab.index(max(probab))
    print(pred_label)

new_test_data_2 = copy.copy(test_data_2)

test_data_2_1, test_data_2_2 = torch.utils.data.random_split(new_test_data_2, [2000, 7000])

print(len(test_data_2_1))

#m = global_client_updates[0][2][3]

tloader = torch.utils.data.DataLoader(test_data_2_1, batch_size = 512, shuffle=True)

# since shuffle=True, this is a random sample of test data
batch = next(iter(tloader))
images, lables = batch
print(images.size())
print(lables.size())

background = images[:500]

batch = next(iter(tloader))
print(len(batch))

#batch1, batch2 = torch.utils.data.random_split(batch, [500, 11])

#e = shap.DeepExplainer(m, background)
#e

#shap_values = e.shap_values(images[501:])

#print(len(shap_values[0]))

img = images[500:]

l = lables[500:]

len(l)

data = []
for model in global_client_updates[0][2]:
  e = shap.DeepExplainer(model, background)
  for index in range(len(img)):
    shap_val = e.shap_values(img[index:index+1])
    for i in range(len(shap_val)):
        data.append((shap_val[i][0],10*l[index]+i))
print(len(data))

train_shap, test_shap = torch.utils.data.random_split(shap_dataset, [3100, 500])

len(shap_dataset)

# defining trainloader and testloader
trloader = torch.utils.data.DataLoader(train_shap, batch_size=32, shuffle=True)
teloader = torch.utils.data.DataLoader(test_shap, batch_size=32, shuffle=True)

# shape of training data
dataiter = iter(trloader)
images, labels = dataiter.next()

print(images.shape)
print(labels.shape)

# defining the model architecture
class Net(nn.Module):   
  def __init__(self):
    super(Net, self).__init__()
    self.conv1 = nn.Conv2d(1, 32, 3, 1)
    self.conv2 = nn.Conv2d(32, 64, 3, 1)
    self.dropout1 = nn.Dropout2d(0.25)
    self.dropout2 = nn.Dropout2d(0.5)
    self.fc1 = nn.Linear(9216, 256)
    self.fc2 = nn.Linear(256, 100)
    
  def forward(self,x):
    x = self.conv1(x)
    x = F.relu(x)
    x = self.conv2(x)
    x = F.relu(x)
    x = F.max_pool2d(x, 2)
    x = self.dropout1(x)
    x = torch.flatten(x, 1)
    x = self.fc1(x)
    x = F.relu(x)
    x = self.dropout2(x)
    x = self.fc2(x)
    output = F.log_softmax(x, dim=1)
    return output

# visualizing the training images
plt.imshow(images[0].numpy().squeeze())

# defining the model
model = Net().float()
# defining the optimizer
optimizer = optim.Adam(model.parameters(), lr=0.001)
# defining the loss function
criterion = nn.CrossEntropyLoss()

    
print(model)

model.train()
for e in range(2):
  running_loss = 0
  for batch_idx, (data, target) in enumerate(trloader):
    optimizer.zero_grad()
    output = model(data.float())
    loss = criterion(output, target)
    loss.backward()
    optimizer.step()
    running_loss += loss.item()
  print("Epoch {} - Training loss: {}".format(e,running_loss/len(trloader)))

# return client update
#return loss.item()

# getting predictions on test set and measuring the performance
correct_count, all_count = 0, 0
for images,labels in teloader:
  for i in range(len(labels)):
    if torch.cuda.is_available():
        images = images.cuda()
        labels = labels.cuda()
    img = images[i].view(1, 1, 28, 28)
    with torch.no_grad():
        logps = model(img.float())

    
    ps = torch.exp(logps)
    probab = list(ps.cpu()[0])
    pred_label = probab.index(max(probab))
    true_label = labels.cpu()[i]
    if(true_label == pred_label):
      correct_count += 1
    all_count += 1

print("Number Of Images Tested =", all_count)
print("\nModel Accuracy =", (correct_count/all_count)*100)

